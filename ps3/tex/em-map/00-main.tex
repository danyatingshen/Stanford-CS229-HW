\item \points{15} {\bf EM for MAP estimation}

The EM algorithm that we talked about in class was for solving a 
maximum likelihood estimation problem in which we wished to maximize
\[
 \prod_{i=1}^\nexp p(\xsi;\theta) = 
 \prod_{i=1}^\nexp \sum_{\zsi} p(\xsi, \zsi;\theta),
\]
where the $\zsi$'s were latent random variables. 
Suppose we are working
in a Bayesian framework, and wanted to find the MAP estimate of the parameters
$\theta$ by maximizing 
\[
 \left( \prod_{i=1}^\nexp p(\xsi|\theta) \right) p(\theta) = 
 \left( \prod_{i=1}^\nexp \sum_{\zsi} p(\xsi, \zsi|\theta)\right) p(\theta).
\]
Here, $p(\theta)$ is our prior on the parameters.  Generalize the EM algorithm to 
work for MAP estimation. You may assume that $\log p(x,z|\theta)$ and $\log p(\theta)$ 
are both concave in $\theta$, so that the M-step is tractable if it 
requires only maximizing a linear combination of these quantities.  
(This roughly corresponds to assuming that MAP estimation is tractable 
when $x,z$ is fully observed, just like in the frequentist case
where we considered examples in which maximum likelihood estimation was easy if $x,z$ was
fully observed.)

Make sure your M-step is tractable, and also prove that 
$\prod_{i=1}^\nexp p(\xsi|\theta) p(\theta)$ (viewed as a function of $\theta$) 
monotonically increases with each iteration of your algorithm.  

\ifnum\solutions=1 {
  \input{em-map/00-main-sol}
} \fi
